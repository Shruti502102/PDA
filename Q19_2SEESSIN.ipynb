{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part A: Calculating TF-IDF for Documents"
      ],
      "metadata": {
        "id": "-GTkvcjGDcDD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8ao47m4DanW"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Given documents\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The fox is quick and jumps high.\",\n",
        "    \"A lazy dog lies under the tree.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Step 2: Fit and transform the documents\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "Pnth_bYxD_G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Convert the result to a DataFrame for better visualization\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the resulting TF-IDF values for each word in each document\n",
        "print(\"TF-IDF values:\")\n",
        "print(tfidf_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2RsZY_NEBfX",
        "outputId": "b84a694e-d830-441d-d684-9e95eb3997f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF values:\n",
            "        and     brown       dog       fox      high        is     jumps  \\\n",
            "0  0.000000  0.398811  0.303306  0.303306  0.000000  0.000000  0.303306   \n",
            "1  0.443503  0.000000  0.000000  0.337295  0.443503  0.443503  0.337295   \n",
            "2  0.000000  0.000000  0.358291  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "       lazy     lies      over     quick       the     tree    under  \n",
            "0  0.303306  0.00000  0.398811  0.303306  0.471089  0.00000  0.00000  \n",
            "1  0.000000  0.00000  0.000000  0.337295  0.261940  0.00000  0.00000  \n",
            "2  0.358291  0.47111  0.000000  0.000000  0.278245  0.47111  0.47111  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part B: Quadratic Regression Model using TensorFlow"
      ],
      "metadata": {
        "id": "fin3fFoyDiXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Generate synthetic data (y = 2x^2 + 3x + 4)\n",
        "x_train = np.linspace(-10, 10, 100)  # 100 points between -10 and 10\n",
        "y_train = 2 * x_train**2 + 3 * x_train + 4 + np.random.normal(0, 10, 100)  # Adding noise"
      ],
      "metadata": {
        "id": "S4YMrKqtDh_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define the quadratic model\n",
        "class QuadraticModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Randomly initialized weights and bias\n",
        "        self.w1 = tf.Variable(np.random.randn(), dtype=tf.float32)\n",
        "        self.w2 = tf.Variable(np.random.randn(), dtype=tf.float32)\n",
        "        self.b = tf.Variable(np.random.randn(), dtype=tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.w1 * x**2 + self.w2 * x + self.b"
      ],
      "metadata": {
        "id": "fP_52mx5EHxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Instantiate the model\n",
        "model = QuadraticModel()\n",
        "\n",
        "# Step 4: Define loss function and optimizer\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "AOjh5rJIELIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Training the model\n",
        "for epoch in range(1000):  # Train for 1000 epochs\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x_train)\n",
        "        loss = loss_fn(y_train, predictions)\n",
        "    # Compute gradients and update weights\n",
        "    gradients = tape.gradient(loss, [model.w1, model.w2, model.b])\n",
        "    optimizer.apply_gradients(zip(gradients, [model.w1, model.w2, model.b]))\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Step 6: Print learned weights and bias\n",
        "print(\"\\nLearned weights and bias:\")\n",
        "print(f\"w1 (x^2 coefficient): {model.w1.numpy()}\")\n",
        "print(f\"w2 (x coefficient): {model.w2.numpy()}\")\n",
        "print(f\"b (bias): {model.b.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2elpPeDDEL6A",
        "outputId": "3bbb0834-25a9-46e5-b5aa-c485d9128d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Loss: 3183.31005859375\n",
            "Epoch 200, Loss: 802.8833618164062\n",
            "Epoch 300, Loss: 214.19456481933594\n",
            "Epoch 400, Loss: 120.4604263305664\n",
            "Epoch 500, Loss: 111.00732421875\n",
            "Epoch 600, Loss: 110.38652038574219\n",
            "Epoch 700, Loss: 110.34178924560547\n",
            "Epoch 800, Loss: 110.31951904296875\n",
            "Epoch 900, Loss: 110.29678344726562\n",
            "Epoch 1000, Loss: 110.27323913574219\n",
            "\n",
            "Learned weights and bias:\n",
            "w1 (x^2 coefficient): 2.03389048576355\n",
            "w2 (x coefficient): 2.9513375759124756\n",
            "b (bias): 4.040149211883545\n"
          ]
        }
      ]
    }
  ]
}